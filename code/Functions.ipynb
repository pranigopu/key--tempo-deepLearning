{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C8QHrXWfcDp"
   },
   "source": [
    "# 0. FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported libraries & modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# For handling data:\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# For handling audio:\n",
    "import librosa\n",
    "\n",
    "# For handling plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For handling graphical display:\n",
    "import IPython.display as ipd\n",
    "\n",
    "# For handling neural networks:\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# For handling file handling: \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some preliminary issues faced & their solutions**\n",
    "\n",
    "**ISSUE**: Dependencies for `keras`\n",
    "\n",
    "I was unable to import `keras` without having `tensorflow` installed.\n",
    "\n",
    "---\n",
    "\n",
    "**ISSUE**: Changing certain OS settings to install `tensorflow`\n",
    "\n",
    "I was unable to install `tensorflow` without referring to the following:\n",
    "\n",
    "https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=powershell#enable-long-paths-in-windows-10-version-1607-and-later\n",
    "\n",
    "I followed the PowerShell solution given.\n",
    "\n",
    "---\n",
    "\n",
    "**ISSUE**: Procedure entry point not in the dynamic link library\n",
    "\n",
    "I faced the following error from the Windows OS (in a dialog box):\n",
    "\n",
    "```\n",
    "The procedure entry point could not be located in the dynamic link library <DDL path>.\n",
    "```\n",
    "\n",
    "NOTE: `<DDL path>` is a placeholder for the actual path.\n",
    "\n",
    "To solve this, I simply restarted and updated the OS. To verify the integrity of the system files, I ran:\n",
    "\n",
    "```\n",
    "sfc /scannow\n",
    "```\n",
    "\n",
    "NOTE: The above needs to be run as an administrator in Command Prompt.\n",
    "\n",
    "This solution was found here...\n",
    "\n",
    "https://www.drivereasy.com/knowledge/fixed-entry-point-not-found-error-in-windows/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining spectrograms & melspectrograms\n",
    "Obtaining spectrograms for each audio file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_spectrograms(df, audio_folder, spectrograms_storage_file_name, sr, n_fft, hop_length, n_frames):\n",
    "    spectrograms = []\n",
    "    spectrograms_by_file_id = {}\n",
    "    \n",
    "    try:\n",
    "        # Obtaining stored spectrograms (if possible):\n",
    "        spectrograms = list(np.load(spectrograms_storage_file_name))\n",
    "        \n",
    "        option = input('Regenerate spectrograms?')\n",
    "        if option == 'Yes':\n",
    "            raise Exception\n",
    "    except:\n",
    "        # Getting the spectrogram for each audio file:\n",
    "        spectrograms = []\n",
    "        \n",
    "        #________________________\n",
    "        # FOR PROGRESS BAR:\n",
    "        prev_i = 0 \n",
    "        max_i = len(df['TRACK'])\n",
    "        #________________________\n",
    "        \n",
    "        for i, file_id in enumerate(df['TRACK']):\n",
    "            #________________________\n",
    "            # FOR PROGRESS BAR:\n",
    "            if i // (max_i/12) > prev_i // (max_i/12):\n",
    "                print('.', end='')\n",
    "                prev_i = i\n",
    "            #________________________\n",
    "            \n",
    "            # Loading the file using `librosa`:\n",
    "            signal, _ = librosa.load(audio_folder + '/' + str(file_id) + '.LOFI' +'.mp3', sr=sr)\n",
    "            # NOTE 1: 'Signal' here indicates the audio file as a whole.\n",
    "            # NOTE 2: `audio_folder` was defined earlier.\n",
    "            \n",
    "            # Short-time Fourier transform:\n",
    "            stft = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_fft)\n",
    "    \n",
    "            # Obtain the spectrogram:\n",
    "            spectrogram = np.abs(stft)\n",
    "    \n",
    "            #........................\n",
    "            # Evening out irregularities in dimensions...\n",
    "            \n",
    "            # Pad spectrogram if necessary:\n",
    "            if spectrogram.shape[1] < n_frames:\n",
    "                spectrogram = np.pad(spectrogram, ((0, 0), (0, n_frames-spectrogram.shape[1])))\n",
    "            # Truncate spectrogram if necessary\n",
    "            spectrogram = spectrogram[:, :n_frames] \n",
    "            #........................\n",
    "            \n",
    "            # Storing the spectrogram data:\n",
    "            spectrograms.append(spectrogram)\n",
    "    \n",
    "        print('\\nDone')\n",
    "        # NOTE ON PROGRESS BAR: It is designed such that maximum length is 12\n",
    "        \n",
    "        # Save the spectrogram data:\n",
    "        np.save(spectrograms_storage_file_name, np.array(spectrograms))\n",
    "    \n",
    "    #================================================\n",
    "    # Storing spectrograms by file ID for easy access later:\n",
    "    for file_id, spectrogram in zip(df['TRACK'], spectrograms):\n",
    "        spectrograms_by_file_id[file_id] = spectrogram\n",
    "\n",
    "    return spectrograms_by_file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY FOCUS ON FINDING AND STORING SPECTROGRAMS?**\n",
    "\n",
    "In the course of this project, I wanted to experiment with different kinds and configurations of melspectrograms. However, the constant values were always the basic spectrograms on which functions were applied to obtain melspectrograms. Since loading audio files is the most time-consuming part of the process of obtaining useful data for the audio files, I decided to minimise the loading of audio files by loading them only once to obtain the spectrograms, after which it becomes easy to access and work on the audio-related data to experiment as I desire. For this reason, we generate or try to generate spectrograms first and foremost. The function of our interest for future use is `get_melspectrogram` (defined later), and regenerating melspectrograms once the basic spectrograms are available shall be much faster.\n",
    "\n",
    "---\n",
    "\n",
    "Obtaining melspectrograms for each audio file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get melspectrogram for a given audio file ID:\n",
    "def get_melspectrogram(file_id, spectrograms_by_file_id, sr, n_fft, hop_length, n_mels):\n",
    "    spectrogram = spectrograms_by_file_id[file_id]\n",
    "    \n",
    "    # Converting the above to log-scaled amplitudes:\n",
    "    log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "    # Melspectrogram spectrogram with log-scaled amplitudes:\n",
    "    melspectrogram = librosa.feature.melspectrogram(S=log_spectrogram, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "    return melspectrogram\n",
    "\n",
    "#================================================\n",
    "# Load all melspectrograms for our purpose, else if not available then create them:\n",
    "def get_all_melspectrograms(df, audio_folder, spectrograms_storage_file_name, melspectrograms_storage_file_name,\n",
    "                            segments_per_file=4, sr=44100, n_fft=2048, hop_length=512, n_mels=64, n_frames=2000):\n",
    "    '''\n",
    "    df: Dataframe containing necessary annotated data\n",
    "    audio_folder: Folder containing the audio files\n",
    "    segments_per_file: Number of segments to divide each file into\n",
    "    spectrograms_storage_file_name: Name of the file in which spectrograms are to be stored\n",
    "    melspectrograms_storage_file_name: Name of the file in which melspectrograms are to be stored\n",
    "    \n",
    "    sr: Sampling rate\n",
    "    n_fft: Number of sinusoids to check for in FFT (i.e. FFT size)\n",
    "    hop_length: Hop length\n",
    "    n_files: Number of audio files\n",
    "    segments_per_file: Number of audio file segments\n",
    "    n_mels: Number of mel bands to be used\n",
    "    n_frames: Number of frames to divide each audio file into\n",
    "    '''\n",
    "\n",
    "    # Obtaining necessary data:\n",
    "    spectrograms_by_file_id = get_all_spectrograms(df, audio_folder, spectrograms_storage_file_name, sr, n_fft, hop_length, n_frames) # Dictionary of spectrograms associated to file IDs\n",
    "    n_files = len(df) # Number of files being considered = Number of rows in the dataframe\n",
    "    n_segments = n_files * segments_per_file # Segments in total (all files combined)\n",
    "    \n",
    "    try:\n",
    "        # Obtaining stored melspectrograms (if possible):\n",
    "        data = list(np.load(melspectrograms_storage_file_name))\n",
    "        \n",
    "        option = input('Regenerate melspectrograms?')\n",
    "        if option == 'Yes':\n",
    "            raise Exception\n",
    "    except:\n",
    "        # Getting the melspectrogram for each audio file:\n",
    "        \n",
    "        data = []\n",
    "    \n",
    "        #________________________\n",
    "        # FOR PROGRESS BAR:\n",
    "        prev_i = 0 \n",
    "        #________________________\n",
    "        \n",
    "        for i, file_id in enumerate(df['TRACK']):\n",
    "            #________________________\n",
    "            # FOR PROGRESS BAR:\n",
    "            if i//(n_files/12) > prev_i//(n_files/12):\n",
    "                print('.', end='')\n",
    "                prev_i = i\n",
    "            #________________________\n",
    "            \n",
    "            # Obtain the spectrogram for the current audio file:\n",
    "            melspectrogram = get_melspectrogram(file_id, spectrograms_by_file_id, sr, n_fft, hop_length, n_mels)\n",
    "        \n",
    "            # Store a previously set number of segments of the melspectrogram obtained:\n",
    "            for i in range(segments_per_file):\n",
    "                data.append(melspectrogram[:, i*(n_frames//segments_per_file):(i+1)*(n_frames//segments_per_file)])\n",
    "            # NOTE: Target labels are stored in `df['TARGET']`\n",
    "    \n",
    "        print('\\nDone')\n",
    "        # NOTE ON PROGRESS BAR: It is designed such that maximum length is 12\n",
    "        \n",
    "        # Save the melspectrograms for later:\n",
    "        np.save(melspectrograms_storage_file_name, np.array(data))\n",
    "\n",
    "    # Parameters:\n",
    "    params = {}\n",
    "    params['segments_per_file'] = segments_per_file\n",
    "    params['sr'] = sr\n",
    "    params['n_fft'] = n_fft\n",
    "    params['hop_length'] = hop_length\n",
    "    params['n_mels'] = n_mels\n",
    "    params['n_frames'] = n_frames\n",
    "\n",
    "    return params, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the items in `data` (for verifying the code's success)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_melspectrogram(data, sr=44100, hop_length=512):\n",
    "    # Checking a random melspectrogram from `data`...\n",
    "    librosa.display.specshow(data[np.random.randint(0, len(data))], sr=sr, hop_length=hop_length)\n",
    "    plt.title('Melspectrogram')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Mel bands')\n",
    "    plt.colorbar()\n",
    "    # NOTE: Mel bands are represented by colour\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets\n",
    "Preparing data for the following:\n",
    "\n",
    "- Viewing and working the data and target labels in simple formats\n",
    "- Working with neural networks (abstracting aspects like batches and data shuffling)\n",
    "\n",
    "**SOME NOTES**:\n",
    "\n",
    "- `to_categorical` was imported as `from tensorflow.keras.utils import to_categorical`\n",
    "- `to_categorical` converts integer labels to the appropriate 1-hot encoding\n",
    "- The below is mostly to increases convenience; we can do without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data (along with the corresponding labels of course):\n",
    "def get_shuffled_data(df, data, segments_per_file):\n",
    "    # Total labels:\n",
    "    labels = []\n",
    "    for label in df['TARGET']:\n",
    "        labels += [label]*segments_per_file\n",
    "    \n",
    "    # Shuffling the data for unbiased training and testing (hence better convergence of model):\n",
    "    # Joining melspectrograms and labels to shuffle data and labels in corresponding order...\n",
    "    D = list(zip(data, labels))\n",
    "    # Shuffling list items...\n",
    "    random.shuffle(D)\n",
    "    # Separating melspectograms and their labels for future convenience...\n",
    "    shuffled_data = np.array([d[0] for d in D])\n",
    "    shuffled_labels = np.array([d[1] for d in D])\n",
    "\n",
    "    return shuffled_data, shuffled_labels\n",
    "\n",
    "#================================================\n",
    "# Dividing the data and labels into training and validation datasets:\n",
    "def get_data_in_splits(data, labels, validation_start):\n",
    "    # Specifying proportions for datasets:\n",
    "    validation_start = round(validation_start*len(labels)) # Might as well be `len(data)`\n",
    "    \n",
    "    # Training data:\n",
    "    train_data = data[:validation_start] # Feature values\n",
    "    train_labels = labels[:validation_start] # Target values\n",
    "    \n",
    "    # Testing data:\n",
    "    validation_data = data[validation_start:] # Feature values\n",
    "    validation_labels = labels[validation_start:] # Target values\n",
    "    \n",
    "    print(f'Training data shape = {train_data.shape}, Validation data shape = {validation_data.shape}')\n",
    "\n",
    "    return train_data, train_labels, validation_data, validation_labels\n",
    "\n",
    "#================================================\n",
    "# Get datasets wrapped in a `tf.data.Dataset` object for convenience when working with neural networks:\n",
    "def get_data(df, data, num_classes, segments_per_file=4, validation_start=0.7, batch_size=32):\n",
    "    # NOTE: `num_classes` = Number of target classes\n",
    "    \n",
    "    data, labels = get_shuffled_data(df, data, segments_per_file)\n",
    "    \n",
    "    # Dividing the data and labels into training and validation datasets:\n",
    "    train_data, train_labels, validation_data, validation_labels = get_data_in_splits(data, labels, validation_start)\n",
    "    \n",
    "    #------------------------------------\n",
    "    # Dictionary of training and validation data and labels in simpler data types:\n",
    "    data_and_labels = {}\n",
    "    data_and_labels['train_data'] = train_data\n",
    "    data_and_labels['validation_data'] = validation_data\n",
    "    data_and_labels['train_labels'] = train_labels\n",
    "    data_and_labels['validation_labels'] = validation_labels\n",
    "\n",
    "    #------------------------------------\n",
    "    # Preparing the dataset for working in neural networks:\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data, to_categorical(train_labels, num_classes=num_classes)))\n",
    "    '''\n",
    "    NOTE:\n",
    "    Shuffling rows in training dataset helps in making the model converge in training.\n",
    "    However, this is not necessary in our case since out dataset was already shuffled before.\n",
    "    However, if it were necessary, we would have done it as follows:\n",
    "    \n",
    "    `train_dataset = train_dataset.shuffle(buffer_size=1024)`\n",
    "    '''\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    \n",
    "    # Preparing the testing dataset:\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((validation_data, to_categorical(validation_labels, num_classes=num_classes)))\n",
    "    validation_dataset = validation_dataset.batch(batch_size)\n",
    "\n",
    "    # Parameters:\n",
    "    params = {}\n",
    "    params['segments_per_file'] = segments_per_file\n",
    "    params['validation_start'] = validation_start\n",
    "    params['num_classes'] = num_classes\n",
    "    params['batch_size'] = batch_size\n",
    "\n",
    "    return params, data_and_labels, train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE ON SHUFFLING DATA BEFORE DIVIDING IT**:\n",
    "\n",
    "Shuffling the data before dividing it into training and testing datasets reduced overfitting and improved the model's accuracy (training and validation). Hence, it seems the original dataset's rows were arranged in a certain order with respect to which the model could overfit; shuffling the rows avoids this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    W = {}\n",
    "    for i, weights in enumerate(model.get_weights()):\n",
    "        W[i] = weights\n",
    "    np.save(file_name, W)\n",
    "\n",
    "def load_model(model, file_name):\n",
    "    V = np.load(file_name, allow_pickle=True).tolist()\n",
    "    W = []\n",
    "    for i in range(len(V)):\n",
    "        W.append(V[i])\n",
    "    model.set_weights(W)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
